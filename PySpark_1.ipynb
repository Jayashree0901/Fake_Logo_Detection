{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f18fce2-13e0-4aa8-a4b0-05c0247c2f25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType, StringType, StructType, StructField\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ResizeImages\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the main directory containing \"true\" and \"fake\" subdirectories\n",
    "input_directory = '/dbfs/mnt/projectteam5/New_Dataset/'\n",
    "\n",
    "# Path to the directory where resized images will be saved (local file system path)\n",
    "output_directory = '/dbfs/mnt/projectteam5/Pyspark_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resize function\n",
    "def resize_image(image_path, width=250, height=250):\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (width, height))\n",
    "    # Encode the resized image back into binary format\n",
    "    retval, buffer = cv2.imencode('.jpg', resized_img)\n",
    "    resized_image_data = buffer.tobytes()\n",
    "    return resized_image_data\n",
    "\n",
    "# Register the UDF (User Defined Function)\n",
    "resize_image_udf = udf(resize_image, BinaryType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image file paths in the input directory\n",
    "image_paths = []\n",
    "for label in os.listdir(input_directory):\n",
    "    label_dir = os.path.join(input_directory, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for file in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a schema for the DataFrame\n",
    "schema = StructType([StructField(\"image_path\", StringType(), nullable=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the image file paths and schema\n",
    "image_paths_df = spark.createDataFrame([(path,) for path in image_paths], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the resize function to the DataFrame\n",
    "resized_images_df = image_paths_df.withColumn(\"resized_image\", resize_image_udf(image_paths_df[\"image_path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a Pandas DataFrame\n",
    "pandas_df = resized_images_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resized images using OpenCV\n",
    "for index, row in pandas_df.iterrows():\n",
    "    image_data = np.frombuffer(row['resized_image'], dtype=np.uint8)\n",
    "    img = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
    "    label = os.path.basename(os.path.dirname(row['image_path']))\n",
    "    cv2.imwrite(os.path.join(output_directory, f\"{label}_resized_image_{index}.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_images_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
